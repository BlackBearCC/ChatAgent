import time

import graphsignal
from langchain_community.document_loaders import CSVLoader, JSONLoader, TextLoader
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.vectorstores.chroma import Chroma
from langchain_community.vectorstores.milvus import Milvus

from simpleaichat import prompt
from simpleaichat.ai_generator import LocalLLMGenerator, OpenAIGenerator, QianWenGenerator
from simpleaichat.data_factory import extract_and_save_as_json

from simpleaichat.document_splitter.text_splitter import TextSplitter, RecursiveCharacterTextSplitter
import graphsignal
import asyncio
graphsignal.configure(api_key='f2ec8486fa256a498ef9272ad9981422', deployment='my-model-prod-v1')
# from simpleaichat.embedding.huggingface import HuggingFaceBgeEmbeddings

from langchain_community.graphs.graph_document import GraphDocument
from langchain_community.graphs.graph_document import Node, Relationship
from langchain_core.documents import Document
NEO4J_URI="neo4j+s://159d31d7.databases.neo4j.io"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="bKOuLr5ZGAGjFC-VMm1wonVhk1f3konW9OAEh0g8J-A"
AURA_INSTANCEID="159d31d7"
AURA_INSTANCENAME="Instance01"

from langchain.graphs import Neo4jGraph

# print(re)
def task_completed_notification():
    print("----------------------æ•°æ®å­˜å‚¨ä»»åŠ¡å®Œæˆ----------------------")
    data_get()


def embedding_scores(scores):
    print("åµŒå…¥å¾—åˆ†ï¼š", scores)


    while True:
        try:
            llm_output = llm.generate(data_prompt)
            break
        except Exception as e:
            print(f"ç”Ÿæˆå¤±è´¥: {e}")
            print("å°è¯•é‡æ–°è¿æ¥...")
            time.sleep(3)

    # File path for the output JSON file
    output_file_path = '/simpleaichat/extracted_data.json'
    extract_and_save_as_json(llm_output, output_file_path, callback=task_completed_notification)


graphdb = Neo4jGraph(url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD)

npcName= "çº¢å¿ƒçš‡å"

query = f"""
MATCH (n)-[r]->(m)
WHERE n.npcName = '{npcName}'
RETURN n, r, m
LIMIT 10
"""
document_source = Document(
    page_content="åŠ¨æ€æ¸¸æˆä¿¡æ¯",
    metadata={"author": "leozy", "date": "2024"}
)
result = graphdb.query(query)
user = Node(id="1", type="user", properties={"name": "å¤§å¤´"})
charactor = Node(id="2", type="charactor", properties={"name": "å…”å½"})
friendship = Relationship(source=user, target=charactor, type="FRIENDS_WITH", properties={"since": "2024"})
graph_doc = GraphDocument(nodes=[user, charactor], relationships=[friendship], source=document_source)
graphdb.add_graph_documents([graph_doc])


print(result)
csvloader = CSVLoader(file_path="game_env.csv", autodetect_encoding=True)

textLoader = TextLoader(file_path="game_env_dec.txt", autodetect_encoding=True)
# jsonloader = JSONLoader(file_path="ç¦ç”¨äººç‰©.json", jq_schema="question" ,text_content=True)
# loader = TextLoader(file_path= "ç¯å¢ƒæè¿°.txt",autodetect_encoding= True)

# loader = JSONLoader(
#     file_path='D:\AIAssets\ProjectAI\simpleaichat\TuJi.json',
#     jq_schema='.question.response',
#     text_content=False)
documents_env = csvloader.load()  # åŒ…å«å…ƒæ•°æ®çš„æ–‡æ¡£åˆ—è¡¨
documents_env_dec = textLoader.load()  # åŒ…å«å…ƒæ•°æ®çš„æ–‡æ¡£åˆ—è¡¨
# documents_people = jsonloader.load()  # åŒ…å«å…ƒæ•°æ®çš„æ–‡æ¡£åˆ—è¡¨
text_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=10)
documents_env = text_splitter.split_documents(documents_env)
documents_env_dec = text_splitter.split_documents(documents_env_dec)
# documents_people = text_splitter.split_documents(documents_people)

model_name = "thenlper/gte-small-zh"  # é˜¿é‡ŒTGE
# model_name = "BAAI/bge-small-zh-v1.5" # æ¸…åBGE
encode_kwargs = {'normalize_embeddings': True}
embedding_model = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs={'device': 'cpu'},
    encode_kwargs=encode_kwargs
)
vectordb = Chroma.from_documents(documents=documents_env, embedding=embedding_model)

csvloader.file_path = "æ—¥å¸¸é—®å€™.csv"
vectordb.add_documents(csvloader.load())
csvloader.file_path = "ä¼ ç»ŸèŠ‚æ—¥.csv"
vectordb.add_documents(csvloader.load())
csvloader.file_path = "äºŒåå››èŠ‚æ°”.csv"
vectordb.add_documents(csvloader.load())
# csvloader.file_path = "ä¸–ç•Œè®¾å®š.csv"
# vectordb.add_documents(csvloader.load())

vectordb.add_documents(documents_env_dec)
textLoader = TextLoader(file_path="ç¦ç”¨äººç‰©.txt", autodetect_encoding=True)
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)
documents_people = textLoader.load()  # åŒ…å«å…ƒæ•°æ®çš„æ–‡æ¡£åˆ—è¡¨
documents_people = text_splitter.split_documents(documents_people)
vectordb.add_documents(documents_people)

intention_llm = LocalLLMGenerator()

topic_llm = LocalLLMGenerator()
# test = OpenAIGenerator()
generator = QianWenGenerator()

gpu_server_generator = LocalLLMGenerator()

chat_history = ["None"]
topic_history = []
intent_history = []

reference = "None"
user_name = "å“¥å“¥"
char_name = "å…”å½"
intention = ""
entity_user = user_name
entity_char = char_name
entity_user_summary = ""
entity_char_summary = ""
user_info = "[å…´è¶£:é˜…è¯»], [æ€§æ ¼:å†…å‘], [è¿‘æœŸæƒ…æ„Ÿ:æ­£å¸¸]"
char_emotion = "[æƒ…ç»ªçŠ¶æ€:æ­£å¸¸]"
char_info = f"[å…´è¶£:é˜…è¯»ç«¥è¯ä¹¦], [æ€§æ ¼:å†…å‘ï¼Œå®³ç¾], {char_emotion}ï¼Œ[ç”Ÿç†çŠ¶æ€:æ­£å¸¸],[ä½ç½®ï¼šå®¢å…]ï¼Œ[åŠ¨ä½œï¼šç«™ç«‹]"

summary = ""
summary_history= ""

user_profile = "[å…´è¶£:é˜…è¯»], [æ€§æ ¼:å†…å‘], [è¿‘æœŸæƒ…æ„Ÿ:æ­£å¸¸]"
extracted_triplets = [("ç”¨æˆ·", "æ— æ˜ç¡®éœ€æ±‚")]
dialogue_situation = """
èƒŒæ™¯å’Œç¯å¢ƒï¼š
åœ¨ä¸€ä¸ªæ¸©é¦¨çš„å®¢å…å†…ï¼Œé˜³å…‰é€è¿‡çª—æˆ·æ´’ä¸‹ï¼Œå°†æ•´ä¸ªç©ºé—´æ¸²æŸ“æˆæ¸©æš–çš„è‰²è°ƒã€‚å®¢å…é‡Œæ‘†æ”¾ç€æŸ”è½¯çš„æ²™å‘å’Œè‰²å½©æ–‘æ–“çš„æŠ±æ•ï¼Œåˆ›é€ å‡ºä¸€ä¸ªæ”¾æ¾å’Œèˆ’é€‚çš„ç¯å¢ƒã€‚ä¸ä»…å¦‚æ­¤ï¼Œæˆ¿é—´ä¸­è¿˜å¸ƒæ»¡äº†æ¢¦å¹»èˆ¬çš„è£…é¥°ï¼šå°å–‡å­ã€å¤é“œè‰²è½åœ°ç¯ã€é­”æ³•å°çŒªé“¶è¡Œï¼Œä»¥åŠå……æ»¡ç«¥è¶£çš„å¤§ç™½å–µå’Œå°å…”å›¾æ¡ˆåœ°æ¯¯ã€‚è¿™ä¸ä»…æ˜¯ä¸€ä¸ªå®¢å…ï¼Œæ›´åƒæ˜¯ä¸€ä¸ªå……æ»¡æ•…äº‹å’Œæ¢¦æƒ³çš„å°ä¸–ç•Œã€‚
å¯¹è¯å†…å®¹æ‘˜è¦ï¼š
åœ¨è¿™æ¬¡å†’é™©çš„å¼€å§‹ï¼Œ{char}å’Œ{user}åœ¨å……æ»¡æ¢¦å¹»çš„å®¢å…ä¸­ç›¸é‡ã€‚{char}ï¼ŒåŸæ˜¯ä¸€ä¸ªç«¥è¯ä¸–ç•Œä¸­çš„å°é…è§’ï¼Œé€šè¿‡ç¥ç§˜åŠ›é‡è¿›å…¥äº†{user}æ‰€åœ¨çš„ä¸–ç•Œã€‚åœ¨è¿™ä¸ªå…¨æ–°çš„ç¯å¢ƒä¸­ï¼Œ{char}è¡¨ç°å‡ºå¥½å¥‡å’Œæ¿€åŠ¨ï¼Œè€Œ{user}åˆ™æ˜¾å¾—æœ‰äº›å›°æƒ‘ä½†ä¹Ÿä¹äºæ¥å—è¿™ä¸ªæ„å¤–çš„ä¼™ä¼´ã€‚ç»è¿‡ä¸€ç³»åˆ—çš„äº’åŠ¨å’Œæ¢ç´¢ï¼Œä»–ä»¬å»ºç«‹äº†å‹è°Šï¼Œå¹¶ä¸€èµ·åˆ¶ä½œé£Ÿç‰©ã€æ¢ç´¢ç§æ¤é—´ï¼Œå‘ç°äº†æ¼‚æµ®çš„éœ²ç ï¼Œå¹¶ç”¨å®ƒåˆ¶ä½œäº†ç¥å¥‡çš„é¦™é¦™æ±½æ°´ã€‚
è§’è‰²è®¾å®šå’Œç‰¹æ€§ï¼š
{char}ï¼šå¥½å¥‡å¿ƒå¼ºï¼Œå……æ»¡æ´»åŠ›ï¼Œæ¸´æœ›å†’é™©å’Œæ–°ç»å†ã€‚ä½œä¸ºåŸç«¥è¯æ•…äº‹ä¸­çš„å°é…è§’ï¼Œ{char}å¸Œæœ›åœ¨æ–°çš„ä¸–ç•Œä¸­æˆä¸ºä¸»è§’ï¼Œå¹¶æ¢ç´¢å±äºè‡ªå·±çš„æ•…äº‹ã€‚
{user}ï¼šå†·é™ï¼Œç†æ€§ï¼Œé€‚åº”èƒ½åŠ›å¼ºã€‚ä½œä¸ºç°å®ä¸–ç•Œä¸­çš„æ™®é€šäººï¼Œ{user}æ„å¤–åœ°æ¥å—äº†{char}å’Œæ–°çš„å†’é™©ï¼Œå±•ç°å‡ºæ‚ é—²å’Œé€‚åº”çš„æ€åº¦ã€‚
{user}è¡Œä¸ºå’Œæƒ…ç»ªå€¾å‘ï¼š
{user}åœ¨æ•´ä¸ªäº’åŠ¨è¿‡ç¨‹ä¸­ä¿æŒç€å¼€æ”¾å’Œæ¥çº³çš„å¿ƒæ€ã€‚å°½ç®¡{char}çš„çªç„¶å‡ºç°å’Œå¥‡å¦™çš„äº‹ä»¶è®©{user}æ„Ÿåˆ°æƒŠè®¶ï¼Œä½†ä»–è¿˜æ˜¯æ„¿æ„æ¥çº³å¹¶å¸®åŠ©{char}é€‚åº”è¿™ä¸ªæ–°ä¸–ç•Œã€‚åœ¨åˆ¶ä½œé£Ÿç‰©å’Œæ¢ç´¢ç§æ¤é—´çš„è¿‡ç¨‹ä¸­ï¼Œ{user}å±•ç°äº†é¢†å¯¼å’Œå…³æ€€çš„ä¸€é¢ï¼Œå¼•å¯¼{char}å‘ç°æ–°äº‹ç‰©ï¼Œå¹¶ä¸€èµ·è§£å†³é—®é¢˜ã€‚
å½“å‰å¯¹è¯çš„å…³é”®ç‚¹å’Œç›®æ ‡ï¼š
å½“å‰å¯¹è¯çš„å…³é”®ç‚¹åœ¨äºåŠ å¼º{char}å’Œ{user}ä¹‹é—´çš„å‹æƒ…ï¼Œå…±åŒæ¢ç´¢è¿™ä¸ªæ–°ä¸–ç•Œï¼Œå¹¶ä¸ºæ¥ä¸‹æ¥çš„å†’é™©å¥ å®šåŸºç¡€ã€‚ç›®æ ‡æ˜¯è®©{char}æ›´å¥½åœ°é€‚åº”æ–°ç¯å¢ƒï¼ŒåŒæ—¶è®©{user}æ›´åŠ äº†è§£{char}çš„æ€§æ ¼å’Œéœ€æ±‚ã€‚é€šè¿‡è¿™æ¬¡äº’åŠ¨ï¼Œä»–ä»¬å¯ä»¥å‘ç°æ›´å¤šå…³äºè¿™ä¸ªæ–°ä¸–ç•Œçš„ç§˜å¯†ï¼Œå¹¶å‡†å¤‡å¥½é¢å¯¹å³å°†åˆ°æ¥çš„æŒ‘æˆ˜å’Œå†’é™©ã€‚"""

impression = "[ç¤¼è²Œ][å‹å¥½]"

chat_content = ""
# ANSIè½¬ä¹‰åºåˆ—
ORANGE = '\033[33m'
GREEN = '\033[32m'
RESET = '\033[0m'

entity_db = Chroma.from_documents(documents=documents_people, embedding=embedding_model)
dialogue_situation = dialogue_situation.format(char=char_name, user=user_name)

# æ„å›¾è¯†åˆ«å›è°ƒ
def callback_intention(content, usage):
    # print(f"{ORANGE}ğŸ”·ğŸ”·ğŸ”·ç”Ÿæˆæ–‡æœ¬ğŸ”·ğŸ”·ğŸ”·\n{text}{RESET}")
    global intention
    intention = content
    # print(f"{GREEN}\nğŸ“>è¾…åŠ©æ„å›¾>>>>>{content}{RESET}")



# å‚è€ƒèµ„æ–™å›è°ƒ
def callback_rag_summary(content, usage):
    if content == "FALSE":
        print(f"{ORANGE}ğŸ”·ğŸ”·ğŸ”·å‚è€ƒèµ„æ–™ğŸ”·ğŸ”·ğŸ”·\n***æ²¡æœ‰åˆé€‚çš„å‚è€ƒèµ„æ–™ï¼Œéœ€æ›´åŠ æ³¨æ„å›ç­”æ—¶çš„äº‹å®ä¾æ®ï¼é¿å…å¹»è§‰ï¼***{RESET}")
    else:
        global reference
        reference = content
        print(f"{GREEN}\nğŸ“‘>èµ„æ–™å®ä½“>>>>>Entity Identification:\n{content}{RESET}")


async def callback_chat(content):
    global chat_content
    global impression
    task = ""
    head_idx = 0
    print(f"{GREEN}\nğŸ“‘>Chain of thought>>>>>:{RESET}")
    print(f"{GREEN}ğŸ®>GameData(sample)>>>>>:{char_info}{RESET}")
    for resp in content:
        paragraph = resp.output['text']
        # ç¡®ä¿æŒ‰å­—ç¬¦è€Œéå­—èŠ‚æ‰“å°
        for char in paragraph[head_idx:]:
            # æ‰“å°è“è‰²å­—ä½“
            print("\033[34m{}\033[0m".format(char), end='', flush=True)
            # æ¯ä¸ªå­—ç¬¦æ‰“å°åæš‚åœ0.1ç§’
            # time.sleep(0.01)
        head_idx = len(paragraph)
        # å¦‚æœæ®µè½ä»¥æ¢è¡Œç¬¦ç»“æŸï¼Œä¿ç•™è¯¥ä½ç½®
        if paragraph.endswith('\n'):
            head_idx -= 1
    # æ›´æ–°å·²æ‰“å°çš„å­—ç¬¦ä½ç½®

    chat_content = paragraph
    parts = paragraph.split("FINAL_ANSWER")
    if len(parts) > 1:
        answer_parts = parts[1].split("TASK")
        # if answer_parts:
        chat_content = f"{char_name}{parts[1].strip()}"

        impression_part = chat_content.split("\n")
        if len(impression_part) > 1:
            task = impression_part[1].strip()
            print(f"{GREEN}\nğŸ“>TASK>>>>>{task}{RESET}")

            # cleaned_text = re.sub(r'[^a-zA-Z]', '', answer_parts[1].strip())
    # print(f"{GREEN}\nâ›“FINAL>>>>>>{chat_content}{RESET}")
    chat_history.append(f'{user_name}ï¼š{query}')
    chat_history.append(chat_content)
    intent_history.append(chat_content)
    if "è®°å¿†æ›´æ–°" in task:
        # æ¦‚è¦æç¤º
        prompt_summary = prompt.DEFAULT_SUMMARIZER_TEMPLATE.format(new_lines=chat_history, summary=summary,
                                                                   user=user_name, char=char_name)
        # å®ä½“è¯†åˆ«
        prompt_entity = prompt.DEFAULT_ENTITY_SUMMARIZATION_TEMPLATE.format(history=chat_history,
                                                                            summary=f"{entity_user}:{entity_user_summary}",
                                                                            entity=f"{entity_user}",
                                                                            input=chat_history)
        await generator.async_sync_call_streaming(prompt_entity, callback=callback_entity_summary)
        # await generator.async_sync_call_streaming(prompt_summary, callback=callback_summary)
    if "æƒ…å¢ƒæ›´æ–°" in task:
        # æƒ…å¢ƒæ¨¡æ‹Ÿ
        prompt_simulation = prompt.AGENT_SIMULATION.format(dialogue_situation=dialogue_situation,
                                                           dialogue_excerpt=chat_history,
                                                           user=user_name, char=char_name)
        await generator.async_sync_call_streaming(prompt_simulation, callback=callback_simulation)
    if "æƒ…ç»ªæ›´æ–°" in task:
        # æƒ…ç»ª
        prompt_emotion = prompt.AGENT_EMOTION.format(emotion=char_emotion,
                                                     dialogue_situation=dialogue_situation,
                                                     history=chat_history,
                                                     char=char_name)
        await generator.async_sync_call_streaming(prompt_emotion, callback=callback_emotion)
async def typewriter(content):
    head_idx = 0
    for resp in content:
        paragraph = resp.output['text']
        # ç¡®ä¿æŒ‰å­—ç¬¦è€Œéå­—èŠ‚æ‰“å°
        for char in paragraph[head_idx:]:
            # æ‰“å°è“è‰²å­—ä½“
            print("\033[34m{}\033[0m".format(char), end='', flush=True)
            # æ¯ä¸ªå­—ç¬¦æ‰“å°åæš‚åœ0.1ç§’
            # time.sleep(0.01)
        head_idx = len(paragraph)
        # å¦‚æœæ®µè½ä»¥æ¢è¡Œç¬¦ç»“æŸï¼Œä¿ç•™è¯¥ä½ç½®
        if paragraph.endswith('\n'):
            head_idx -= 1
    return paragraph
async def callback_simulation(content):
    global dialogue_situation
    dialogue_situation = content
    # await typewriter(content)
    # print(f"{GREEN}\nğŸ“>æƒ…å¢ƒæ¨¡æ‹Ÿ>>>>>{content}{RESET}")

async def callback_analysis(content):
    await typewriter(content)
    # print(f"{GREEN}\nğŸ“>å¯¹è¯åˆ†æ>>>>>{content}{RESET}")

async def callback_emotion(content):
    global char_emotion
    global char_info
    # char_emotion = content
    char_emotion=  await typewriter(content)

    char_info = f"[å…´è¶£:é˜…è¯»ç«¥è¯ä¹¦], [æ€§æ ¼:å†…å‘ï¼Œå®³ç¾], {char_emotion}ï¼Œ[ç”Ÿç†çŠ¶æ€:æ­£å¸¸],[ä½ç½®ï¼šå®¢å…]ï¼Œ[åŠ¨ä½œï¼šç«™ç«‹]"
async def callback_summary(content):
    global summary
    summary = content
    await typewriter(content)
    entity_db.add_texts(content)
    # print(f"{GREEN}\nğŸ“>å¯¹è¯æ¦‚è¦>>>>>{content}{RESET}")

async def callback_entity_summary(content):
    global entity_user_summary
    entity_user_summary = content
    print(f"{GREEN}\nğŸ“>å®ä½“æ›´æ–°>>>>>{entity_user_summary}{RESET}")
    await typewriter(content)
    # print(f"{GREEN}\nğŸ“>å®ä½“è¯†åˆ«>>>>>{entity_user_summary}{RESET}")

@graphsignal.trace_function
#å†³ç­–æ¨¡å‹
async def decision_agent(prompt_decision):
    await generator.async_sync_call_streaming(prompt_decision, callback=callback_chat)


async def async_sync_call_streaming(prompt_simulation):
    # è¿™é‡Œå‡è®¾ generator.sample_sync_call_streaming å¯ä»¥ç›´æ¥ä½œä¸ºå¼‚æ­¥è°ƒç”¨
    # å¦‚æœä¸æ˜¯ï¼Œä½ å¯èƒ½éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°ä¸­ä½¿ç”¨å…¶ä»–çš„å¼‚æ­¥é€”å¾„æ¥è°ƒç”¨å®ƒ
    await generator.async_sync_call_streaming(prompt_simulation, callback=callback_simulation)
print(f"{GREEN}\nğŸ“>å½“å‰æƒ…å¢ƒ>>>>>{dialogue_situation}{RESET}")
print(f"{GREEN}\nğŸ“>äº‹ä»¶>>>>><äº‹ä»¶>çŒªé³„å˜å‡ºäº†é‡‘å¸ï¼Œå“¥å“¥å’Œå…”å½å¾—åˆ°ä¸€äº›é‡‘å¸ï¼Œä½†çŒªé³„é™åˆ¶äº†æ•°é‡ã€‚{RESET}")



while True:
    # è¾“å…¥

    query = input("\nè¾“å…¥: ")
    # æ„å›¾è¯†åˆ«
    intention_prompt = f"{prompt.INTENTION}\n é—®:{intent_history}{query}\né¢„æœŸè¾“å‡º:"
    gpu_server_generator.generate_normal(intention_prompt, callback=callback_intention)
    intent_history.append(f'é—®ï¼š{query}')
    docs = vectordb.similarity_search_with_score(intention)
    entity_doc = entity_db.similarity_search_with_score(user_name)
    entity_contents = []
    for doc, score in entity_doc:
        # å°†æ¯ä¸ªæ–‡æ¡£çš„å†…å®¹å’Œå®ƒçš„å¾—åˆ†æ·»åŠ åˆ°page_contentsåˆ—è¡¨
        if score < 0.3:
            entity_contents.append(f"{doc.page_content} (å¾—åˆ†: {score})")
            print(f"{GREEN}\nğŸ“‘>å®ä½“è¯†åˆ«>>>>>{doc.page_content}{RESET}")


    # å¯¹è¯æƒ…æ„Ÿæ£€ç´¢
    # å¯¹è¯ä¸»é¢˜æ£€ç´¢
    # å¯¹è¯ç‰¹å¾æ£€ç´¢

    # ç›´æ¥æ£€ç´¢

    page_contents = []
    for doc, score in docs:
        # å°†æ¯ä¸ªæ–‡æ¡£çš„å†…å®¹å’Œå®ƒçš„å¾—åˆ†æ·»åŠ åˆ°page_contentsåˆ—è¡¨
        if score < 0.35:
            page_contents.append(f"{doc.page_content} (å¾—åˆ†: {score})")

    if len(page_contents):
        combined_contents = '\n'.join(page_contents)
        print(f"{ORANGE}ğŸ“‘>å‚è€ƒèµ„æ–™>>>>>\n{combined_contents}{RESET}")
        # reference = combined_contents

        # # å‚è€ƒèµ„æ–™å®ä½“æ¦‚æ‹¬
        # rag_summary = prompt.AGENT_RAG_ENTITY.format(reference=combined_contents)  # æš‚æ—¶ä¸æ¦‚æ‹¬
        # gpu_server_generator.generate_normal(rag_summary, callback=callback_rag_summary)  # æš‚æ—¶ä¸æ¦‚æ‹¬

    else:
        combined_contents = "***æ²¡æœ‰åˆé€‚çš„å‚è€ƒèµ„æ–™ï¼Œéœ€æ›´åŠ æ³¨æ„å›ç­”æ—¶çš„äº‹å®ä¾æ®ï¼é¿å…å¹»è§‰ï¼***"
        # print(f"{ORANGE}ğŸ“‘âŒ>å‚è€ƒèµ„æ–™>>>>>æœªè¯†åˆ«åˆ°æœ‰æ•ˆèµ„æ–™ï¼Œéœ€æ›´åŠ æ³¨æ„å›ç­”æ—¶çš„äº‹å®ä¾æ®ï¼é¿å…å¹»è§‰ï¼***{RESET}")




    # ç”Ÿæˆ
    # try:
    #     # final_prompt = f"{prompt.COSER}\n {prompt.RAG}\nå‚è€ƒèµ„æ–™:\n{combined_contents}\nå†å²è®°å½•ï¼š{chat_history}\n{prompt.AGENT_REACT}\n{prompt.REACT_FEW_SHOT}\nå¼€å§‹\nuser:{query}\nå…”å½:"
    #     # final_prompt = prompt.AGENT_REACT.format(impression= impression,history2=chat_history, reference=combined_contents, input=query,user=user_name,char=char_name)
    #     # result = generator.generate_with_rag(final_prompt)
    #     # final_prompt = prompt.AGENT_REACT_ALL.format( input=query, user=user_name,
    #     #                                          char=char_name)
    #
    #     # generator.sample_sync_call_streaming(final_prompt, callback=callback_chat)
    #
    #
    #
    #     # final_answer = result.get_final_answer()
    #     # topic_changed = result.get_topic_changed()
    #     #
    #     # text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)
    #     # # res = text_splitter.split_text(result.get_final_answer())
    #     #
    #     # if topic_changed == "TRUE":
    #     #     print(f"{ORANGE}ğŸ”·ğŸ”·ğŸ”·Topic ChangedğŸ”·ğŸ”·ğŸ”·{RESET}")
    #     #
    #     #     topic_or_activity = ""
    #     #     summary = ""
    #     #     topic_prompt = prompt.TOPIC.format(history2=topic_history, topic_or_activity=topic_or_activity,
    #     #                                        summary=summary, input=topic_history[-1])
    #     #     topic_llm.generate_normal(topic_prompt)
    #     #     print(f"{ORANGE}ğŸ”·ğŸ”·ğŸ”·Recent Topic ExtractionğŸ”·ğŸ”·ğŸ”·\n{topic_llm.get_response_text()}{RESET}")
    #     #
    #     #     topic_history.clear()
    #     # else:
    #     #     print(f"{ORANGE}â¬œâ¬œâ¬œTopic Not Changeâ¬œâ¬œâ¬œ{RESET}")
    #     #     topic_history.append(f'userï¼š{query}')
    #     #     topic_history.append(f'å…”å½ï¼š{final_answer}')
    #     #
    #     # print(f"æ–‡æœ¬åˆ†å‰²:{res}")
    #     # vectordb.add_texts(res)
    #     #
    #     # entity_db.add_texts(res)
    #     #
    #     # # print(vectordb.add_texts(res))
    #     #
    #     # # print(chat_history)
    #     # intent_history.append(f'ç­”ï¼š{final_answer}')
    # except ValueError as e:
    #     print(e)
    # except Exception as e:
    #     print(e)


    async def main():
        global char_info
        global user_info
        # # æ¦‚è¦æç¤º
        # prompt_summary = prompt.DEFAULT_SUMMARIZER_TEMPLATE.format(new_lines=chat_history, summary=summary, user=user_name, char=char_name)
        # # å®ä½“è¯†åˆ«
        # prompt_entity = prompt.DEFAULT_ENTITY_SUMMARIZATION_TEMPLATE.format(history2=chat_history,
        #                                                                     summary=entity_user_summary, entity_user=entity_user,
        #                                                                     input=chat_history)
        # # æƒ…å¢ƒæ¨¡æ‹Ÿ
        # prompt_simulation = prompt.AGENT_SIMULATION.format(dialogue_situation=dialogue_situation, dialogue_excerpt=chat_history,
        #                                                    user=user_name, char=char_name)
        # å†³ç­–æ¨¡å‹
        prompt_decision = prompt.AGENT_DECISION.format(user_profile=user_profile,
                                                       dialogue_situation=dialogue_situation,
                                                       extracted_triplets=extracted_triplets,
                                                       chat_history=chat_history,
                                                       user=user_name, char=char_name, input=query)

        # prompt_analysis = prompt.AGENT_ANALYSIS.format(history2=chat_history,user= user_name,char=char_name,input=query,reference=combined_contents)

        # char_info = ("[å…´è¶£:é˜…è¯»ç«¥è¯ä¹¦], [æ€§æ ¼:å†…å‘ï¼Œå®³ç¾], [æƒ…ç»ªçŠ¶æ€:ç”Ÿæ°”"
        #              "   ]ï¼Œ[ç”Ÿç†çŠ¶æ€:é¥¥é¥¿],[ä½ç½®ï¼šå®¢å…]ï¼Œ[åŠ¨ä½œï¼šç«™ç«‹]...")
        prompt_game = prompt.AGENT_ROLE_TEST.format(user=user_name,user_info=user_info,
                                                    char=char_name,char_info=char_info,
                                                    input=query,dialogue_situation=dialogue_situation,
                                                    reference=combined_contents,lines_history=chat_history,summary_history=summary)
        # await generator.async_sync_call_streaming(prompt_analysis, callback=callback_analysis)
        await generator.async_sync_call_streaming(prompt_game, callback=callback_chat)
        # char_info = "[å…´è¶£:é˜…è¯»ç«¥è¯ä¹¦], [æ€§æ ¼:å†…å‘ï¼Œå®³ç¾], [æƒ…ç»ªçŠ¶æ€:å¥½å¥‡]ï¼Œ[ç”Ÿç†çŠ¶æ€:æ­£å¸¸],[ä½ç½®ï¼šå¨æˆ¿]ï¼Œ[åŠ¨ä½œï¼šç«™ç«‹]"
        # prompt_game = prompt.AGENT_ROLE.format(user=user_name, user_info=user_info, char=char_name, char_info=char_info,
        #                                        input=query, dialogue_situation=dialogue_situation,
        #                                        reference=combined_contents, history2=chat_history)
        # await generator.async_sync_call_streaming(prompt_game, callback=callback_chat)
        # await generator.async_sync_call_streaming(prompt_entity, callback=callback_entity_summary)
        # await generator.async_sync_call_streaming(prompt_summary, callback=callback_summary)
        # await generator.async_sync_call_streaming(prompt_simulation, callback=callback_simulation)
        # await generator.async_sync_call_streaming(prompt_decision, callback=callback_chat)


    # è¿è¡Œä¸»å‡½æ•°
    asyncio.run(main())


