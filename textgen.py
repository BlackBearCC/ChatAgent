# -*- coding:utf-8 -*-
import time

from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.vectorstores.chroma import Chroma

from app import prompt
from app.ai_generator import LocalLLMGenerator, QianWenGenerator
from app.data_factory import extract_and_save_as_json

from app.utils.document_splitter.text_splitter import RecursiveCharacterTextSplitter
import graphsignal
import asyncio

from app.database.leo_neo4j_graph import DatabaseConfig, Leo_Neo4jGraph

graphsignal.configure(api_key='f2ec8486fa256a498ef9272ad9981422', deployment='my-model-prod-v1')

from langchain_community.graphs.graph_document import GraphDocument
from langchain_community.graphs.graph_document import Node, Relationship

from langchain_core.documents import Document

from app.models import UserProfile
from app.models import CharacterProfile
import re  # å¯¼å…¥ re æ¨¡å—
from langchain_community.llms import Tongyi
from app.utils.data_loader import DataLoader
import json
from app.core.prompts.tool_prompts import search_helper, search_graph_helper
from fastapi import FastAPI


def split_text(documents, chunk_size, chunk_overlap):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return text_splitter.split_documents(documents)


def embedding_scores(scores):
    print("åµŒå…¥å¾—åˆ†ï¼š", scores)

    while True:
        try:
            llm_output = llm.generate(data_prompt)
            break
        except Exception as e:
            print(f"ç”Ÿæˆå¤±è´¥: {e}")
            print("å°è¯•é‡æ–°è¿æ¥...")
            time.sleep(3)

    # File path for the output JSON file
    output_file_path = 'app/extracted_data.json'
    extract_and_save_as_json(llm_output, output_file_path, callback=task_completed_notification)


import databases

DATABASE_URL = "mysql+mysqlconnector://<username>:<password>@<host>/<dbname>"
database = databases.Database(DATABASE_URL)
# metadata = sqlalchemy.MetaData()


app = FastAPI()
# Base = declarative_base()

query = ""
data_config = DatabaseConfig("config.ini")
try:
    graphdb = Leo_Neo4jGraph(data_config.neo4j_uri, data_config.neo4j_username, data_config.neo4j_password)
except Exception as e:
    print(e)

documents_env = DataLoader("game_env.csv").load()
documents_env_dec = DataLoader("game_env_dec.txt").load()

documents_env = split_text(documents_env, 50, 10)
documents_env_dec = split_text(documents_env_dec, 50, 10)

model_name = "thenlper/gte-small-zh"  # é˜¿é‡ŒTGE
# model_name = "BAAI/bge-small-zh-v1.5" # æ¸…åBGE
encode_kwargs = {'normalize_embeddings': True}
embedding_model = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs={'device': 'cpu'},
    encode_kwargs=encode_kwargs
)
vectordb = Chroma.from_documents(documents=documents_env, embedding=embedding_model)

files = ["æ—¥å¸¸é—®å€™.csv", "ä¼ ç»ŸèŠ‚æ—¥.csv", "äºŒåå››èŠ‚æ°”.csv", "ç¦ç”¨äººç‰©.txt"]

for file in files:
    documents = DataLoader(file).load()
    vectordb.add_documents(documents)

intention_llm = LocalLLMGenerator()

topic_llm = LocalLLMGenerator()
# test = OpenAIGenerator()
generator = QianWenGenerator()

gpu_server_generator = LocalLLMGenerator()

from app.service.service import *

# å‡è®¾çš„ä¼šè¯ID
session_id = "123"

# è°ƒç”¨æœåŠ¡å±‚å‡½æ•°
user_profile, character_profile = get_user_and_character_profiles(session_id)
dialogue_manager = get_dialogue_manager_service(session_id)
print(update_dialogue_summary_service(session_id, "æ¦‚è¦æµ‹è¯•"))
print(get_dialogue_summary_service(session_id))
print(update_dialogue_situation_service(session_id, "æƒ…å¢ƒæµ‹è¯•"))
print(get_dialogue_situation_service(session_id))

print(dialogue_manager.situation)

# ä½¿ç”¨è·å–åˆ°çš„æ•°æ®
if user_profile and character_profile:
    print("User Name:", user_profile.name)
    print("Character Name:", character_profile.name)

from app.service.service import *

# extracted_triplets = [("ç”¨æˆ·", "æ— æ˜ç¡®éœ€æ±‚")]
default_dialogue_situation = """
èƒŒæ™¯å’Œç¯å¢ƒï¼š
åœ¨ä¸€ä¸ªæ¸©é¦¨çš„å®¢å…å†…ï¼Œé˜³å…‰é€è¿‡çª—æˆ·æ´’ä¸‹ï¼Œå°†æ•´ä¸ªç©ºé—´æ¸²æŸ“æˆæ¸©æš–çš„è‰²è°ƒã€‚å®¢å…é‡Œæ‘†æ”¾ç€æŸ”è½¯çš„æ²™å‘å’Œè‰²å½©æ–‘æ–“çš„æŠ±æ•ï¼Œåˆ›é€ å‡ºä¸€ä¸ªæ”¾æ¾å’Œèˆ’é€‚çš„ç¯å¢ƒã€‚ä¸ä»…å¦‚æ­¤ï¼Œæˆ¿é—´ä¸­è¿˜å¸ƒæ»¡äº†æ¢¦å¹»èˆ¬çš„è£…é¥°ï¼šå°å–‡å­ã€å¤é“œè‰²è½åœ°ç¯ã€é­”æ³•å°çŒªé“¶è¡Œï¼Œä»¥åŠå……æ»¡ç«¥è¶£çš„å¤§ç™½å–µå’Œå°å…”å›¾æ¡ˆåœ°æ¯¯ã€‚è¿™ä¸ä»…æ˜¯ä¸€ä¸ªå®¢å…ï¼Œæ›´åƒæ˜¯ä¸€ä¸ªå……æ»¡æ•…äº‹å’Œæ¢¦æƒ³çš„å°ä¸–ç•Œã€‚
å¯¹è¯å†…å®¹æ‘˜è¦ï¼š
åœ¨è¿™æ¬¡å†’é™©çš„å¼€å§‹ï¼Œ{char}å’Œ{user}åœ¨å……æ»¡æ¢¦å¹»çš„å®¢å…ä¸­ç›¸é‡ã€‚{char}ï¼ŒåŸæ˜¯ä¸€ä¸ªç«¥è¯ä¸–ç•Œä¸­çš„å°é…è§’ï¼Œé€šè¿‡ç¥ç§˜åŠ›é‡è¿›å…¥äº†{user}æ‰€åœ¨çš„ä¸–ç•Œã€‚åœ¨è¿™ä¸ªå…¨æ–°çš„ç¯å¢ƒä¸­ï¼Œ{char}è¡¨ç°å‡ºå¥½å¥‡å’Œæ¿€åŠ¨ï¼Œè€Œ{user}åˆ™æ˜¾å¾—æœ‰äº›å›°æƒ‘ä½†ä¹Ÿä¹äºæ¥å—è¿™ä¸ªæ„å¤–çš„ä¼™ä¼´ã€‚ç»è¿‡ä¸€ç³»åˆ—çš„äº’åŠ¨å’Œæ¢ç´¢ï¼Œä»–ä»¬å»ºç«‹äº†å‹è°Šï¼Œå¹¶ä¸€èµ·åˆ¶ä½œé£Ÿç‰©ã€æ¢ç´¢ç§æ¤é—´ï¼Œå‘ç°äº†æ¼‚æµ®çš„éœ²ç ï¼Œå¹¶ç”¨å®ƒåˆ¶ä½œäº†ç¥å¥‡çš„é¦™é¦™æ±½æ°´ã€‚
è§’è‰²è®¾å®šå’Œç‰¹æ€§ï¼š
{char}ï¼šå¥½å¥‡å¿ƒå¼ºï¼Œå……æ»¡æ´»åŠ›ï¼Œæ¸´æœ›å†’é™©å’Œæ–°ç»å†ã€‚ä½œä¸ºåŸç«¥è¯æ•…äº‹ä¸­çš„å°é…è§’ï¼Œ{char}å¸Œæœ›åœ¨æ–°çš„ä¸–ç•Œä¸­æˆä¸ºä¸»è§’ï¼Œå¹¶æ¢ç´¢å±äºè‡ªå·±çš„æ•…äº‹ã€‚
{user}ï¼šå†·é™ï¼Œç†æ€§ï¼Œé€‚åº”èƒ½åŠ›å¼ºã€‚ä½œä¸ºç°å®ä¸–ç•Œä¸­çš„æ™®é€šäººï¼Œ{user}æ„å¤–åœ°æ¥å—äº†{char}å’Œæ–°çš„å†’é™©ï¼Œå±•ç°å‡ºæ‚ é—²å’Œé€‚åº”çš„æ€åº¦ã€‚
{user}è¡Œä¸ºå’Œæƒ…ç»ªå€¾å‘ï¼š
{user}åœ¨æ•´ä¸ªäº’åŠ¨è¿‡ç¨‹ä¸­ä¿æŒç€å¼€æ”¾å’Œæ¥çº³çš„å¿ƒæ€ã€‚å°½ç®¡{char}çš„çªç„¶å‡ºç°å’Œå¥‡å¦™çš„äº‹ä»¶è®©{user}æ„Ÿåˆ°æƒŠè®¶ï¼Œä½†ä»–è¿˜æ˜¯æ„¿æ„æ¥çº³å¹¶å¸®åŠ©{char}é€‚åº”è¿™ä¸ªæ–°ä¸–ç•Œã€‚åœ¨åˆ¶ä½œé£Ÿç‰©å’Œæ¢ç´¢ç§æ¤é—´çš„è¿‡ç¨‹ä¸­ï¼Œ{user}å±•ç°äº†é¢†å¯¼å’Œå…³æ€€çš„ä¸€é¢ï¼Œå¼•å¯¼{char}å‘ç°æ–°äº‹ç‰©ï¼Œå¹¶ä¸€èµ·è§£å†³é—®é¢˜ã€‚
å½“å‰å¯¹è¯çš„å…³é”®ç‚¹å’Œç›®æ ‡ï¼š
å½“å‰å¯¹è¯çš„å…³é”®ç‚¹åœ¨äºåŠ å¼º{char}å’Œ{user}ä¹‹é—´çš„å‹æƒ…ï¼Œå…±åŒæ¢ç´¢è¿™ä¸ªæ–°ä¸–ç•Œï¼Œå¹¶ä¸ºæ¥ä¸‹æ¥çš„å†’é™©å¥ å®šåŸºç¡€ã€‚ç›®æ ‡æ˜¯è®©{char}æ›´å¥½åœ°é€‚åº”æ–°ç¯å¢ƒï¼ŒåŒæ—¶è®©{user}æ›´åŠ äº†è§£{char}çš„æ€§æ ¼å’Œéœ€æ±‚ã€‚é€šè¿‡è¿™æ¬¡äº’åŠ¨ï¼Œä»–ä»¬å¯ä»¥å‘ç°æ›´å¤šå…³äºè¿™ä¸ªæ–°ä¸–ç•Œçš„ç§˜å¯†ï¼Œå¹¶å‡†å¤‡å¥½é¢å¯¹å³å°†åˆ°æ¥çš„æŒ‘æˆ˜å’Œå†’é™©ã€‚"""

impression = "[ç¤¼è²Œ][å‹å¥½]"

prompt_test = prompt.prompt_test.format(char=character_profile.name, user=user_profile.name)

# ANSIè½¬ä¹‰åºåˆ—
ORANGE = '\033[33m'
GREEN = '\033[32m'
RESET = '\033[0m'

dialogue_manager.situation = default_dialogue_situation.format(char=character_profile.name, user=user_profile.name)


# æ„å›¾è¯†åˆ«å›è°ƒ
def callback_intention(content, usage):
    # print(f"{ORANGE}ğŸ”·ğŸ”·ğŸ”·ç”Ÿæˆæ–‡æœ¬ğŸ”·ğŸ”·ğŸ”·\n{text}{RESET}")
    # global intention
    typewriter(content)
    dialogue_manager.intention = content

    # print(f"{GREEN}\nğŸ“>è¾…åŠ©æ„å›¾>>>>>{content}{RESET}")


# å‚è€ƒèµ„æ–™å›è°ƒ
def callback_rag_summary(content, usage):
    if content == "FALSE":
        print(f"{ORANGE}ğŸ”·ğŸ”·ğŸ”·å‚è€ƒèµ„æ–™ğŸ”·ğŸ”·ğŸ”·\n***æ²¡æœ‰åˆé€‚çš„å‚è€ƒèµ„æ–™ï¼Œéœ€æ›´åŠ æ³¨æ„å›ç­”æ—¶çš„äº‹å®ä¾æ®ï¼é¿å…å¹»è§‰ï¼***{RESET}")
    else:
        global reference
        reference = content
        print(f"{GREEN}\nğŸ“‘>èµ„æ–™å®ä½“>>>>>Entity Identification:\n{content}{RESET}")


async def callback_knowledge_graph(content):
    # global testText
    # testText = content
    full_content = await typewriter(content)
    # npcName= "çº¢å¿ƒçš‡å"
    # query = f"""
    # MATCH (n)-[r]->(m)
    # WHERE n.npcName = '{npcName}'
    # RETURN n, r, m
    # LIMIT 10
    # """
    document_source = Document(
        page_content="åŠ¨æ€æ¸¸æˆä¿¡æ¯",
        metadata={"author": "leozy", "date": "2024"}
    )
    # result = graphdb.query(query)

    # user = Node(id="å¤§å¤´", type="user", properties={"name": "å¤§å¤´"})
    # charactor = Node(id="å…”å½", type="charactor", properties={"name": "å…”å½"})
    # friendship = Relationship(source=user, target=charactor, type="FRIENDS_WITH", properties={"since": "2024"})
    # graph_doc = GraphDocument(nodes=[user, charactor], relationships=[friendship], source=document_source)

    graph_document = process_entities_and_relationships(full_content)
    graphdb.add_graph_documents([graph_document])
    print(f"{GREEN}\nğŸ“‘>å›¾è°±>>>>>:\n{full_content}{RESET}")


def clean_json_data(data: str) -> str:
    """
    æ¸…ç† JSON æ•°æ®å­—ç¬¦ä¸²ï¼Œç§»é™¤å¤šä½™çš„å­—ç¬¦ã€‚
    :param data: åŸå§‹æ•°æ®å­—ç¬¦ä¸²ã€‚
    :return: æ¸…ç†åçš„å­—ç¬¦ä¸²ã€‚
    """
    # ç§»é™¤å¯èƒ½çš„å¤šä½™å­—ç¬¦ï¼Œä¾‹å¦‚ï¼š```json å’Œ ```
    data = re.sub(r'^\s*```json\s*|\s*```\s*$', '', data, flags=re.MULTILINE)
    # å¤„ç†å…¶ä»–å¯èƒ½çš„æ ¼å¼é—®é¢˜ï¼ˆæ ¹æ®å®é™…æƒ…å†µæ·»åŠ ï¼‰
    # ...
    print(data)
    return data


def process_entities_and_relationships(data: str) -> GraphDocument:
    try:
        # æ¸…ç†æ•°æ®å¹¶è§£æ JSON
        clean_data = clean_json_data(data)
        data_dict = json.loads(clean_data)

        # æå–å®ä½“å’Œå…³ç³»
        entities_data = data_dict.get("å®ä½“", [])
        relationships_data = data_dict.get("å…³ç³»", [])

        # åˆ›å»ºèŠ‚ç‚¹å¯¹è±¡
        nodes = {entity['id']: Node(id=entity['id'], type=entity['type'], properties={"name": entity['name']})
                 for entity in entities_data}

        # åˆ›å»ºå…³ç³»å¯¹è±¡
        relationships = []
        for rel in relationships_data:
            source_node = nodes.get(rel['source_id'])
            target_node = nodes.get(rel['target_id'])
            if source_node and target_node:
                relationships.append(Relationship(source=source_node, target=target_node, type=rel['type']))

        # åˆ›å»ºæ–‡æ¡£æ¥æº
        document_source = Document(page_content="åŠ¨æ€æ¸¸æˆä¿¡æ¯", metadata={"author": "leozy", "date": "2024"})

        # åˆ›å»ºå¹¶è¿”å› GraphDocument å¯¹è±¡
        print("å›¾è°±æ„å»ºå®Œæˆ---------")
        return GraphDocument(nodes=list(nodes.values()), relationships=relationships, source=document_source)
    except json.JSONDecodeError as e:
        raise ValueError(f"è§£æ JSON æ—¶å‡ºé”™ï¼š{e}")


import langchain.callbacks as callbacks
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser


##é€šä¹‰æµå¼ä¼ è¾“å¤±è´¥
async def generate_stream(llm, prompt, **kwargs):
    async for chunk in llm.generate(
            prompts=["Generate a story about a cat named Mittens."],
            max_tokens=100,
            stream=True
    ):
        print(chunk)


from colorama import Fore, Style


class ChatCallbackHandler(BaseCallbackHandler):
    def on_text(self, text, **kwargs):
        # ä¿®æ”¹å­—ä½“é¢œè‰²
        print(Fore.RED + text + Style.RESET_ALL)

    def on_llm_end(self, response, **kwargs):
        # è°ƒç”¨ä½ æƒ³è¦æ‰§è¡Œçš„å‡½æ•°
        # parser = LLMResultParser()

        print({"response": response})


async def update_entity():
    # å®ä½“è¯†åˆ«
    llm = Tongyi(model_name="qwen-max-1201", top_p=0.1, dashscope_api_key="sk-dc356b8ca42c41788717c007f49e134a")
    entity_template = prompt.DEFAULT_ENTITY_SUMMARIZATION_TEMPLATE
    entity_prompt = PromptTemplate(template=entity_template, input_variables=["history", "summary", "entity", "input"])
    callback_handler = ChatCallbackHandler()
    output_parser = StrOutputParser()
    entity_chain = LLMChain(llm=llm, prompt=entity_prompt, output_parser=output_parser)
    entity_input = {"history": dialogue_manager.chat_history,
                    "summary": dialogue_manager.entity_summary,
                    "entity": dialogue_manager.user_name,
                    "input": dialogue_manager.chat_history}
    entity_result = await entity_chain.ainvoke(entity_input, callbacks=[callback_handler])
    entity_text = entity_result["text"]

    print(f'{GREEN}\nğŸ“>å®ä½“æ›´æ–°>>>>>{entity_text}{RESET}')


async def update_summary(session_id):
    # å¯¹è¯æ¦‚è¦
    llm = Tongyi(model_name="qwen-max-1201", top_p=0.1, dashscope_api_key="sk-dc356b8ca42c41788717c007f49e134a")
    summary_template = prompt.DEFAULT_SUMMARIZER_TEMPLATE
    summary_prompts = PromptTemplate(template=summary_template,
                                     input_variables=["new_lines", "summary", "user", "char"])
    output_parser = StrOutputParser()
    summary_chain = LLMChain(llm=llm, prompt=summary_prompts, output_parser=output_parser)
    summary_input = {"new_lines": dialogue_manager.chat_history,
                     "summary": dialogue_manager.summary,
                     "user": user_info.name,
                     "char": char_info.name}
    summary_result = await summary_chain.ainvoke(summary_input)
    summary_text = summary_result["text"]
    update_dialogue_summary_service(session_id, summary_text)
    print(f'{GREEN}\nğŸ“>å¯¹è¯æ¦‚è¦>>>>>{summary_text}{RESET}')


async def on_update_situation_complete():
    # è¿™é‡Œæ˜¯å›è°ƒå‡½æ•°ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å½“ update_situation() å®Œæˆæ—¶éœ€è¦æ‰§è¡Œçš„ä»£ç 
    print("Update situation completed")


async def update_situation(callback):
    # æƒ…å¢ƒæ¨¡æ‹Ÿ
    llm = Tongyi(model_name="qwen-max-1201", top_p=0.1, dashscope_api_key="sk-dc356b8ca42c41788717c007f49e134a")
    situation_template = prompt.AGENT_SITUATION
    situation_prompt = PromptTemplate(template=situation_template,
                                      input_variables=["dialogue_situation", "dialogue_excerpt", "user", "char"])
    situation_chain = LLMChain(llm=llm, prompt=situation_prompt, output_parser=StrOutputParser())
    situation_input = {"dialogue_situation": dialogue_manager.situation,
                       "dialogue_excerpt": dialogue_manager.chat_history,
                       "user": dialogue_manager.user_name,
                       "char": dialogue_manager.char_name}
    situation_result = await situation_chain.ainvoke(situation_input)
    situation_text = situation_result["text"]
    print(f'{GREEN}\nğŸ“>æƒ…å¢ƒæ¨¡æ‹Ÿ>>>>>{situation_text}{RESET}')
    await callback()
    update_dialogue_situation_service(session_id, situation_text)


async def update_emotion(session_id):
    # æƒ…ç»ª
    llm = Tongyi(model_name="qwen-max-1201", top_p=0.1, dashscope_api_key="sk-dc356b8ca42c41788717c007f49e134a")
    emotion_template = prompt.AGENT_EMOTION
    emotion_prompt = PromptTemplate(template=emotion_template,
                                    input_variables=["emotion", "dialogue_situation", "history", "char"])
    emotion_chain = LLMChain(llm=llm, prompt=emotion_prompt, output_parser=StrOutputParser())
    emotion_input = {"emotion": char_info.emotional_state,
                     "dialogue_situation": dialogue_manager.situation,
                     "history": dialogue_manager.chat_history,
                     "char": char_info.name}
    emotion_result = await emotion_chain.ainvoke(emotion_input)
    emotion_text = emotion_result["text"]
    # char_info.emotional_state = emotion_text
    update_character_emotion_service(session_id, emotion_text)
    print(f'{GREEN}\nğŸ“>æƒ…ç»ªæ›´æ–°>>>>>{emotion_text}{RESET}')


async def callback_chat(content):
    task = ""
    head_idx = 0
    # print(f"{GREEN}\nğŸ“‘>Chain of thought>>>>>:{RESET}")
    # print(f"{GREEN}ğŸ®>GameData(sample)>>>>>:{char_info}{RESET}")
    # for resp in content:
    #     paragraph = resp.output['text']
    #     # ç¡®ä¿æŒ‰å­—ç¬¦è€Œéå­—èŠ‚æ‰“å°
    #     for char in paragraph[head_idx:]:
    #         # æ‰“å°è“è‰²å­—ä½“
    #         print("\033[34m{}\033[0m".format(char), end='', flush=True)
    #         # æ¯ä¸ªå­—ç¬¦æ‰“å°åæš‚åœ0.1ç§’
    #         # time.sleep(0.01)
    #     head_idx = len(paragraph)
    #     # å¦‚æœæ®µè½ä»¥æ¢è¡Œç¬¦ç»“æŸï¼Œä¿ç•™è¯¥ä½ç½®
    #     if paragraph.endswith('\n'):
    #         head_idx -= 1
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–JSONéƒ¨åˆ†
    # å°†å­—èŠ‚å¯¹è±¡è§£ç ä¸ºå­—ç¬¦ä¸²
    decoded_text = content.decode('utf-8')
    search_pattern = '"finish_reason":"stop"'
    if search_pattern in decoded_text:
        result = "åŒ¹é…æˆåŠŸï¼Œæµå¼ä¼ è¾“åœæ­¢ï¼š'finish_reason:stop'."
        # æå–JSONå­—ç¬¦ä¸²
        json_str = decoded_text.split('data:', 1)[1].strip()
        # è½¬æ¢ä¸ºJSONå¯¹è±¡
        try:
            data_json = json.loads(json_str)
            # print(data_json['output']['text'])
            ai_message = data_json['output']['text']
            # æŒ‰ç…§ "FINAL_ANSWER" æ‹†åˆ†
            content_parts = ai_message.split("FINAL_ANSWER")
            if len(content_parts) > 1:
                # å¦‚æœå­˜åœ¨ "TASK"ï¼ŒæŒ‰ "TASK" è¿›ä¸€æ­¥æ‹†åˆ†
                task_parts = content_parts[1].split("TASK", 1)
                # è¿‡æ»¤ ";" å’Œ ":"
                final_answer_content = re.sub(r'[;:]', '', task_parts[0].strip())
            else:
                final_answer_content = ""
            print(f"{GREEN}\nâ›“FINAL>>>>>>{final_answer_content}{RESET}")
            await update_entity()

            dialogue_manager.chat_history.append(f'{user_info.name}:{query}')
            dialogue_manager.chat_history.append(f'{char_info.name}:{final_answer_content}')
        except json.JSONDecodeError:
            print("JSONè§£æé”™è¯¯")
            data_json = {}

    else:
        result = "åŒ¹é…å¤±è´¥ï¼Œæµå¼ä¼ è¾“ä¸­ã€‚"
    # print(decoded_text)
    # print(result)

    # chat_content = paragraph
    # parts = paragraph.split("FINAL_ANSWER")
    # if len(parts) > 1:
    #     answer_parts = parts[1].split("TASK")
    #     # if answer_parts:
    #     chat_content = f"{char_info.name}{parts[1].strip()}"
    #
    #     impression_part = chat_content.split("\n")
    #     if len(impression_part) > 1:
    #         task = impression_part[1].strip()
    #         print(f"{GREEN}\nğŸ“>TASK>>>>>{task}{RESET}")
    #
    #         # cleaned_text = re.sub(r'[^a-zA-Z]', '', answer_parts[1].strip())
    # # print(f"{GREEN}\nâ›“FINAL>>>>>>{chat_content}{RESET}")
    # dialogue_manager.chat_history.append(f'{user_info.name}ï¼š{query}')
    # dialogue_manager.chat_history.append(chat_content)
    # dialogue_manager.intent_history.append(chat_content)
    # if "è®°å¿†æ›´æ–°" in task:
    #     # æ¦‚è¦æç¤º
    #     # prompt_summary = prompt.DEFAULT_SUMMARIZER_TEMPLATE.format(new_lines=chat_history, summary=summary,
    #     #                                                            user=user_name, char=char_name)
    #     # å®ä½“è¯†åˆ«
    #     prompt_entity = prompt.DEFAULT_ENTITY_SUMMARIZATION_TEMPLATE.format(history=dialogue_manager.chat_history,
    #                                                                         summary=f"{dialogue_manager.user_name}:{dialogue_manager.entity_summary}",
    #                                                                         entity=f"{dialogue_manager.user_name}",
    #                                                                         input=dialogue_manager.chat_history)
    #     await generator.async_sync_call_streaming(prompt_entity, callback=callback_entity_summary)
    #     # await generator.async_sync_call_streaming(prompt_summary, callback=callback_summary)
    # if "æƒ…å¢ƒæ›´æ–°" in task:
    #     # æƒ…å¢ƒæ¨¡æ‹Ÿ
    #     prompt_simulation = prompt.AGENT_SITUATION.format(dialogue_situation=dialogue_manager.situation,
    #                                                        dialogue_excerpt=dialogue_manager.chat_history,
    #                                                        user=dialogue_manager.user_name,
    #                                                        char=dialogue_manager.char_name)
    #     await generator.async_sync_call_streaming(prompt_simulation, callback=callback_simulation)
    # if "æƒ…ç»ªæ›´æ–°" in task:
    #     # æƒ…ç»ª
    #     prompt_emotion = prompt.AGENT_EMOTION.format(emotion=char_info.emotional_state,
    #                                                  dialogue_situation=dialogue_manager.situation,
    #                                                  history=dialogue_manager.chat_history,
    #                                                  char=char_info.name)
    #     await generator.async_sync_call_streaming(prompt_emotion, callback=callback_emotion)


async def typewriter(content):
    head_idx = 0
    full_text = ""  # ç”¨äºç´¯ç§¯æ‰€æœ‰æ‰“å°çš„æ–‡æœ¬
    for resp in content:
        paragraph = resp.output['text']
        # ç¡®ä¿æŒ‰å­—ç¬¦è€Œéå­—èŠ‚æ‰“å°
        for char in paragraph[head_idx:]:
            # æ‰“å°è“è‰²å­—ä½“
            print("\033[34m{}\033[0m".format(char), end='', flush=True)
            full_text += char  # æ·»åŠ å­—ç¬¦åˆ°å®Œæ•´æ–‡æœ¬
            # æ¯ä¸ªå­—ç¬¦æ‰“å°åæš‚åœ0.1ç§’
            # time.sleep(0.01)
        head_idx = len(paragraph)
        # å¦‚æœæ®µè½ä»¥æ¢è¡Œç¬¦ç»“æŸï¼Œä¿ç•™è¯¥ä½ç½®
        if paragraph.endswith('\n'):
            head_idx -= 1

    return full_text


async def callback_simulation(content):
    dialogue_manager.situation = content
    # await typewriter(content)
    # print(f"{GREEN}\nğŸ“>æƒ…å¢ƒæ¨¡æ‹Ÿ>>>>>{content}{RESET}")


async def callback_analysis(content):
    await typewriter(content)
    # print(f"{GREEN}\nğŸ“>å¯¹è¯åˆ†æ>>>>>{content}{RESET}")


async def callback_emotion(content):
    # char_emotion = content
    char_info.emotional_state = await typewriter(content)

    char_info = f"[å…´è¶£:é˜…è¯»ç«¥è¯ä¹¦], [æ€§æ ¼:å†…å‘ï¼Œå®³ç¾], {char_info.emotional_state}ï¼Œ[ç”Ÿç†çŠ¶æ€:æ­£å¸¸],[ä½ç½®ï¼šå®¢å…]ï¼Œ[åŠ¨ä½œï¼šç«™ç«‹]"


async def callback_summary(content):
    # global summary
    # summary = content

    # entity_db.add_texts(content)
    # decoded_content = content.decode('utf-8')
    # await typewriter(decoded_content)
    # dialogue_manager.summary_history.append(decoded_content)
    print(f'{GREEN}\nğŸ“>å¯¹è¯æ¦‚è¦>>>>>decoded_content{RESET}')
    # print(f"{GREEN}\nğŸ“>å¯¹è¯æ¦‚è¦>>>>>{content}{RESET}")


async def callback_entity_summary(content):
    dialogue_manager.entity_summary = content
    print(f"{GREEN}\nğŸ“>å®ä½“æ›´æ–°>>>>>{dialogue_manager.entity_summary}{RESET}")
    await typewriter(content)
    # print(f"{GREEN}\nğŸ“>å®ä½“è¯†åˆ«>>>>>{entity_user_summary}{RESET}")


@graphsignal.trace_function
# å†³ç­–æ¨¡å‹
async def decision_agent(prompt_decision):
    await generator.async_sync_call_streaming(prompt_decision, callback=callback_chat)


#
# async def async_sync_call_streaming(prompt_simulation):
#     # è¿™é‡Œå‡è®¾ generator.sample_sync_call_streaming å¯ä»¥ç›´æ¥ä½œä¸ºå¼‚æ­¥è°ƒç”¨
#     # å¦‚æœä¸æ˜¯ï¼Œä½ å¯èƒ½éœ€è¦åœ¨è¿™ä¸ªå‡½æ•°ä¸­ä½¿ç”¨å…¶ä»–çš„å¼‚æ­¥é€”å¾„æ¥è°ƒç”¨å®ƒ
#     await generator.async_sync_call_streaming(prompt_simulation, callback=callback_simulation)


print(f"{GREEN}\nğŸ“>å½“å‰æƒ…å¢ƒ>>>>>{dialogue_manager.situation}{RESET}")
print(f"{GREEN}\nğŸ“>äº‹ä»¶>>>>><äº‹ä»¶>çŒªé³„å˜å‡ºäº†é‡‘å¸ï¼Œå“¥å“¥å’Œå…”å½å¾—åˆ°ä¸€äº›é‡‘å¸ï¼Œä½†çŒªé³„é™åˆ¶äº†æ•°é‡ã€‚{RESET}")

from langchain_community.llms.tongyi import generate_with_retry

# import spacy
# nlp = spacy.load('zh_core_web_sm')
# æ·»åŠ è‡ªå®šä¹‰è¯æ±‡
# nlp.tokenizer.pkuseg_update_user_dict(["å…”å½","å“¥å“¥"])
# æ‰§è¡Œä¸€äº›ç®€å•çš„NLPä»»åŠ¡
# doc = nlp("å§å®¤å¨æˆ¿ä¹”å¸ƒæ–¯")
# for ent in doc.ents:
#     # å®ä½“æ–‡æœ¬ï¼Œå¼€å§‹ä½ç½®ï¼Œç»“æŸä½ç½®ï¼Œå®ä½“æ ‡ç­¾
#     print(ent.text, ent.start_char, ent.end_char, ent.label_)
from pydantic import BaseModel


class GenerationRequest(BaseModel):
    data: str  # æ•°æ®æ¨¡å‹


from fastapi.responses import StreamingResponse
import httpx
from sse_starlette.sse import EventSourceResponse
import requests
# @app.post("/stream-input")
# async def stream_input(request: Request):
#     # è¿™é‡Œæ˜¯å¤„ç†æ¥æ”¶åˆ°çš„æµå¼æ•°æ®çš„é€»è¾‘
#     # ä¾‹å¦‚ï¼Œå°†æ•°æ®å­˜å‚¨åœ¨é˜Ÿåˆ—ä¸­
#     pass
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],  # å…è®¸çš„æº
    allow_credentials=True,
    allow_methods=["*"],  # å…è®¸çš„æ–¹æ³•
    allow_headers=["*"],  # å…è®¸çš„å¤´
)


@app.post("/generate/")
async def generate(request: GenerationRequest):
    global query
    query = request.data
    print(query)
    search_help_prompt = search_graph_helper.format(schema="", content=query)
    # intention_prompt = f"{prompt.INTENTION.format(chat_history=dialogue_manager.chat_history,input=query)}"
    # gpu_server_generator.generate_normal(intention_prompt, callback=callback_intention)
    llm = Tongyi(model_name="qwen-max-1201", top_p=0.1, dashscope_api_key="sk-dc356b8ca42c41788717c007f49e134a")
    params = {
        **{"model": llm.model_name},
        **{"top_p": llm.top_p},
    }
    completion = generate_with_retry(llm=llm, prompt=search_help_prompt, **params)
    print(completion)
    dialogue_manager.intention = completion["output"]["text"]
    dialogue_manager.intent_history.append(f'é—®ï¼š{query}')
    docs = vectordb.similarity_search_with_score(query)

    page_contents = []
    for doc, score in docs:
        # å°†æ¯ä¸ªæ–‡æ¡£çš„å†…å®¹å’Œå®ƒçš„å¾—åˆ†æ·»åŠ åˆ°page_contentsåˆ—è¡¨
        if score < 0.4:
            page_contents.append(f"{doc.page_content} (ä½™å¼¦ç›¸ä¼¼åº¦: {score})")

    if len(page_contents):
        combined_contents = '\n'.join(page_contents)
        print(f"{ORANGE}ğŸ“‘>å‚è€ƒèµ„æ–™>>>>>\n{combined_contents}{RESET}")
        # reference = combined_contents

        # # å‚è€ƒèµ„æ–™å®ä½“æ¦‚æ‹¬
        # rag_summary = prompt.AGENT_RAG_ENTITY.format(reference=combined_contents)  # æš‚æ—¶ä¸æ¦‚æ‹¬
        # gpu_server_generator.generate_normal(rag_summary, callback=callback_rag_summary)  # æš‚æ—¶ä¸æ¦‚æ‹¬

    else:
        combined_contents = "***æ²¡æœ‰åˆé€‚çš„å‚è€ƒèµ„æ–™ï¼Œéœ€æ›´åŠ æ³¨æ„å›ç­”æ—¶çš„äº‹å®ä¾æ®ï¼é¿å…å¹»è§‰ï¼***"
    # # å†³ç­–æ¨¡å‹
    # prompt_decision = prompt.AGENT_DECISION.format(user_profile=user_info,
    #                                                dialogue_situation=dialogue_situation,
    #                                                extracted_triplets=dialogue_manager.extracted_triplets,
    #                                                chat_history=dialogue_manager.chat_history,
    #                                                user=user_info.name, char=char_info.name, input=query)
    prompt_knowledge = prompt.KNOWLEDGE_GRAPH.format(text=prompt_test)
    # char_info = ("[å…´è¶£:é˜…è¯»ç«¥è¯ä¹¦], [æ€§æ ¼:å†…å‘ï¼Œå®³ç¾], [æƒ…ç»ªçŠ¶æ€:ç”Ÿæ°”"
    #              "   ]ï¼Œ[ç”Ÿç†çŠ¶æ€:é¥¥é¥¿],[ä½ç½®ï¼šå®¢å…]ï¼Œ[åŠ¨ä½œï¼šç«™ç«‹]...")
    print(f"{GREEN}ğŸ®>GameData(sample)>>>>>:{char_info}{RESET}")
    print(f"{GREEN}ğŸ®>GameData(sample)>>>>>:{user_info}{RESET}")
    prompt_game = prompt.AGENT_ROLE_TEST.format(user=user_info.name, user_info=user_info,
                                                char=char_info.name, char_info=char_info,
                                                input=query, dialogue_situation=dialogue_manager.situation,
                                                user_entity=dialogue_manager.entity_summary,
                                                reference=combined_contents,
                                                lines_history=dialogue_manager.chat_history,
                                                summary_history=dialogue_manager.summary_history)
    print(dialogue_manager.chat_history)
    tasks = [
        update_emotion(),
        update_summary(),
        update_entity(),
    ]
    await asyncio.gather(*tasks)
    # åˆ›å»ºä¸€ä¸ªæ–°çš„ä»»åŠ¡æ¥è¿è¡Œ update_situationï¼Œä¼ é€’å›è°ƒå‡½æ•°
    asyncio.create_task(update_situation(on_update_situation_complete))
    return EventSourceResponse(generator.async_sync_call_streaming(prompt_game, callback=callback_chat))
