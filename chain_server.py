import asyncio
from typing import Union

from langchain.memory import ConversationBufferWindowMemory, ConversationSummaryMemory
from langchain.prompts import PromptTemplate
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.llms import Tongyi
from langchain_core.messages import SystemMessage

from simpleaichat.memory.game_message_history import GameMessageHistory
from simpleaichat import prompt
import os

from langchain_core.output_parsers import StrOutputParser

os.environ["DASHSCOPE_API_KEY"] = "sk-dc356b8ca42c41788717c007f49e134a"
os.environ["OPENAI_API_KEY"] = "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"

NEO4J_URI="neo4j+s://159d31d7.databases.neo4j.io"
NEO4J_USERNAME="neo4j"
NEO4J_PASSWORD="bKOuLr5ZGAGjFC-VMm1wonVhk1f3konW9OAEh0g8J-A"
AURA_INSTANCEID="159d31d7"
AURA_INSTANCENAME="Instance01"

user = "å“¥å“¥"
char = "å…”å½"
history = ""

entity_user_summary = ""
entity_char_summary = ""
user_info = "[å…´è¶£:é˜…è¯»], [æ€§æ ¼:å†…å‘], [è¿‘æœŸæƒ…æ„Ÿ:æ­£å¸¸]"
char_emotion = "[æƒ…ç»ªçŠ¶æ€:æ­£å¸¸]"
char_info = f"[å…´è¶£:æ— ], [æ€§æ ¼:å†…å‘ï¼Œå®³ç¾], {char_emotion}ï¼Œ[ç”Ÿç†çŠ¶æ€:æ­£å¸¸],[ä½ç½®ï¼šå®¢å…]ï¼Œ[åŠ¨ä½œï¼šç«™ç«‹]"
reference = "None"
summary = ""


extracted_triplets = [("ç”¨æˆ·", "æ— æ˜ç¡®éœ€æ±‚")]
template = prompt.AGENT_ROLE_TEST
# template.format(user=user, char=char, history2=history2) # format the template
# line_memory =[]

history2 = ChatMessageHistory()
line_memory = ConversationBufferWindowMemory( k=100,ai_prefix=f"{char}",human_prefix=f"{user}")

default_summary_memory = ["""<äº‹ä»¶>åœ¨ä¸€æ¬¡æ¼”ç»ç«¥è¯æ•…äº‹åï¼Œå¥½å¥‡å¿ƒé©±ä½¿{char}æ¥åˆ°äº†å…”å­æ´å£ï¼Œå‘å¤–æ¢æœ›ã€‚çªå¦‚å…¶æ¥çš„ç¥ç§˜åŠ›é‡å°†å¥¹å¸å…¥æ·±ä¸è§åº•çš„å…”å­æ´ï¼Œå¼€å§‹äº†ä¸€æ®µæœªçŸ¥çš„å†’é™©ã€‚
<äº‹ä»¶>åœ¨è¿™ä¸ªæ—…ç¨‹ä¸­ï¼Œ{char}ä½“éªŒäº†å„ç§æƒ…ç»ªï¼šç´§å¼ ã€å®³æ€•ã€æ¿€åŠ¨ã€å¥½å¥‡â€¦â€¦åœ¨æ··ä¹±çš„æƒ…ç»ªä¸­ï¼Œå¥¹é™·å…¥äº†æ²‰ç¡ã€‚å½“å¥¹åœ¨{user}å®¶çš„é˜æ¥¼ä¸­é†’æ¥æ—¶ï¼Œå¥¹
<äº‹ä»¶>å…”å­å¥³å­©é†’æ¥åï¼Œå‘ç°è‡ªå·±èº«ç©¿äººç±»è¡£æœï¼ŒæƒŠè®¶åœ°å‘ç°è‡ªå·±ä»ä¸€åªå°å…”å­å˜æˆäº†ä¸€ä¸ªå°å¥³å­©ï¼Œè€Œå¥¹æ‰€å¤„çš„é˜æ¥¼ä¹Ÿå¥½åƒè¿›å…¥äº†ä¸€ä¸ªæ–°çš„ä¸–ç•Œ,å¯¹ç€[user]è¡¨ç°å‡ºæƒŠè®¶å’Œå¥½å¥‡ã€‚
<äº‹ä»¶>{char}çªç„¶å˜å›äº†å°å…”å­å½¢æ€ï¼Œä»¤[user]æ„Ÿåˆ°å›°æƒ‘ã€‚
<äº‹ä»¶>çª—å°ä¸Šçš„çŒªå´½å­˜é’±ç½ä»‹ç»è‡ªå·±ä¸ºçŒªé³„ï¼Œå¹¶ç§°å‘¼è‡ªå·±ä¸ºé‡‘å¸é­”æ³•å¸ˆçŒªé³„ã€‚å…”å­å¥³å­©è¯¢é—®æ˜¯å¦[user]æ˜¯è¿™é‡Œçš„ä¸»äººï¼Œ[user]å›ç­”ç§°è‡ªå·±å¯èƒ½åœ¨çˆ·çˆ·çš„é˜æ¥¼ä¸Šç¡ç€äº†ã€‚
<äº‹ä»¶>çŒªé³„å‘Šè¯‰[user]ä»–ä»¬ç°åœ¨èº«å¤„[user]æ‰“å¼€çš„ç«¥è¯ä¹¦ä¸­ï¼ŒåŸæœ‰æ•…äº‹å·²ç»é£åŒ–ï¼Œè€Œ[user]æˆä¸ºäº†æ–°çš„æ‰§ç¬”è€…ã€‚è€Œå…”å­å¥³å­©[char]åˆ™è¢«æè¿°ä¸ºè„±ç¦»äº†åŸæœ‰ç«¥è¯çš„æ—¶ç©ºï¼Œæˆä¸ºè¿™ä¸ªä¸–ç•Œçš„"å¤–æ˜Ÿäºº"ã€‚
<äº‹ä»¶>[user]å¸Œæœ›èƒ½å›åˆ°é˜æ¥¼ï¼Œä½†çŒªé³„è¡¨ç¤ºé˜æ¥¼å·²ç»è¢«æ–°çš„æ—¶ç©ºå–ä»£ï¼Œåªæœ‰åœ¨å®Œæˆæ•…äº‹åæ‰ä¼šå½¢æˆå°é—­æ—¶ç©ºã€‚çŒªé³„é—­ä¸Šäº†çœ¼ç›ï¼Œä¼¼ä¹ä¸å†ä¸[user]äº’åŠ¨ã€‚
<äº‹ä»¶>[user]å’Œ[char]å‘ç°è‡ªå·±åœ¨æ–°çš„æ—¶ç©ºä¸­ï¼Œ[char]å¯»æ±‚æ–°çš„åå­—ï¼Œæœ€ç»ˆè¢«å‘½åä¸º[char]ã€‚
<äº‹ä»¶>[user]å’Œ[char]å†³å®šä¸€èµ·æ¢ç´¢è¿™ä¸ªæ–°çš„ä¸–ç•Œï¼Œå¹¶å¼€å§‹å†’é™©ä¹‹æ—…ã€‚
<äº‹ä»¶>çŒªé³„èƒ½å¤Ÿå˜å‡ºé‡‘å¸ï¼Œ[user]å’Œ[char]å¾—åˆ°ä¸€äº›é‡‘å¸ï¼Œä½†çŒªé³„é™åˆ¶äº†æ•°é‡ã€‚
<äº‹ä»¶>[user]å’Œ[char]ä¸€èµ·åˆ¶ä½œé£Ÿç‰©ï¼Œä¹‹åæ¢ç´¢ç§æ¤é—´ï¼Œå‘ç°æ¼‚æµ®çš„éœ²ç ã€‚
<äº‹ä»¶>[user]å’Œ[char]åˆ¶ä½œäº†é¦™é¦™æ±½æ°´ï¼Œå‘ç°å®ƒå¯ä»¥æ¶ˆé™¤ç–²åŠ³ã€‚"""]

default_summary_memory[0].format(user=user, char=char)
# åˆ›å»ºéƒ¨åˆ†è§£ææ¨¡æ¿
prompt = PromptTemplate(
    template=template,
    input_variables=["summary_history","lines_history","input"],
    partial_variables={"user": user, "char": char,
                       "history": history, "user_info": user_info, "char_info": char_info,
                       "reference": reference, "summary": summary},
)
parser = StrOutputParser()
llm = Tongyi(model_name="qwen-max-1201")

chain = prompt | llm | parser


async def generate(input_content,summary_memory,line_memory):
    chunks = []
    async for chunk in chain.astream({"input": input_content, "summary_history": summary_memory, "lines_history": line_memory}):
        chunks.append(chunk)
        print(chunk, end="", flush=True)
    "".join(chunks)
    result = "".join(chunks)
    parts = result.split("FINAL_ANSWER")
    if len(parts) > 1:
        answer_parts = parts[1].split("TASK")
        # if answer_parts:
        final_content = f"{char}{parts[1].strip()}"
        history2.add_user_message(input_content)
        history2.add_ai_message(final_content)
        system_message = SystemMessage(
            additional_kwargs={"example": "value"},
            content="This is a system message",
            type="system",
        )
        history2.add_message(system_message)


        # line_memory.save_context({"input": input_content}, {"output": final_content})

        # impression_part = chat_content.split("\n")
        # if len(impression_part) > 1:
        #     task = impression_part[1].strip()
        #     print(f"{GREEN}\nğŸ“>TASK>>>>>{task}{RESET}")

            # cleaned_text = re.sub(r'[^a-zA-Z]', '', answer_parts[1].strip())
    # print(f"{GREEN}\nâ›“FINAL>>>>>>{chat_content}{RESET}")

    # intent_history.append(chat_content)

while True:
    asyncio.run(generate(input("\nINPUT: "), default_summary_memory, line_memory))
    line_memory = ConversationBufferWindowMemory(k=100, chat_memory=history2, ai_prefix=f"{char}",
                                                 human_prefix=f"{user}")
    # print(f"\n{history2.messages}")
    print(f"\n{line_memory}")
    summary_memory = ConversationSummaryMemory.from_messages(llm=llm,chat_memory=history2, human_prefix=f"{user}", ai_prefix=f"{char}")

    print(summary_memory.buffer)




# memory = ConversationBufferWindowMemory( k=1, return_messages=True)
# memory.save_context({"input": "hi"}, {"output": "whats up"})
# memory.save_context({"input": "not much you"}, {"output": "not much"})
# memory.load_memory_variables({})
